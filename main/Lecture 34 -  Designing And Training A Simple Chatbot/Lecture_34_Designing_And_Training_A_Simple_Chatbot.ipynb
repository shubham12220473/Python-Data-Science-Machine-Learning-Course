{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV9ssgcMeC2j"
      },
      "source": [
        "# Designing and Training a Simple Chatbot\n",
        "\n",
        "Preprocessing Text Data\n",
        "\n",
        "Before training a chatbot, we need to preprocess the text data by tokenizing the text, removing stop words, and applying stemmin or lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mzJ9BxtiRDl",
        "outputId": "bdbd2b9a-3a98-4626-cbd7-fe3c38462379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YGVi5RLaU9q",
        "outputId": "ee6b278b-a7b5-4a12-b8a5-b992f1387611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                     Questions  \\\n",
            "0                   introduction to the course   \n",
            "1  overview of data science and its importance   \n",
            "2    introduction to the data science workflow   \n",
            "3         key skills and tools in data science   \n",
            "4          where can i find my course videos ?   \n",
            "\n",
            "                                             Answers  \n",
            "0  Welcome to the data science course. Here you w...  \n",
            "1  Data science is crucial for making informed de...  \n",
            "2  The data science workflow includes data collec...  \n",
            "3  Important skills include programming, statisti...  \n",
            "4  You can find all your course videos on the Cip...  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#Importing the dataset\n",
        "data = pd.read_csv('chatbot_dataset.csv')\n",
        "\n",
        "#Preprocessing the dataset\n",
        "nltk.download('punkt')\n",
        "data['Questions'] = data['Questions'].apply(lambda x: ' '.join(nltk.word_tokenize(x.lower())))\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sUTI6guj3Ef"
      },
      "source": [
        "# Vectorizing Text Data\n",
        "\n",
        "We convert the text data into numerical values using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uKNwS-kewo0",
        "outputId": "737267e3-c663-4cca-f1ca-fae6593891cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(48, 112)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(data['Questions'])\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv81r2bTtm3A"
      },
      "source": [
        "# Training a Text Classification Model\n",
        "\n",
        "We use the Naive Bayes Classifier to train the model on the vectorized text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t4iyqN1tXtY",
        "outputId": "5b9c03cb-408f-4191-d79a-e5c737e17696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Training complete.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Questions'],data['Answers'],test_size=0.2,random_state=42)\n",
        "\n",
        "#Create a model pipeline\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "\n",
        "#Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model Training complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P86a2vqWvpgw"
      },
      "source": [
        "# Implementing a function to get Chatbot Responses\n",
        "\n",
        "We write a funtion to process user input and return responses based on the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCcwu3DAvdtu",
        "outputId": "c970bc7b-0896-4b37-aebf-10c811d88f69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seaborn is a Python visualization library based on Matplotlib that provides a high-level interface for drawing attractive statistical graphics. This is covered in the Data Visualization with Seaborn module.\n"
          ]
        }
      ],
      "source": [
        "#Function to get response from the chatbot\n",
        "def get_response(question):\n",
        "  question = ' '.join(nltk.word_tokenize(question.lower()))\n",
        "  answer = model.predict([question])[0]\n",
        "  return answer\n",
        "\n",
        "#Testing the function\n",
        "print(get_response(\"What is NLP?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8o0iSJBxPcM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNyDB/09Id60nul2gNa0dgV",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
